{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toLabaVHm4eY"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFIfFdv9dM9"
      },
      "source": [
        "### configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VftAN6f9dM_"
      },
      "outputs": [],
      "source": [
        "NAME = \"Rifat\" #your name here\n",
        "PROJECT_NAME = \"Fixmatch_Multioutput_Implementation\"\n",
        "MODEL_TYPE = \"self_supervised\"\n",
        "ARCHITECTURE_NAME = \"tf_efficientnet_lite0\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiSKda8_9dM_"
      },
      "source": [
        "## Install dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzXedWDu9dNA"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm \n",
        "!pip install -q --upgrade --force-reinstall --no-deps kaggle\n",
        "!pip install -q opencv-python-headless==4.1.2.30  \n",
        "!pip install -q  --upgrade wandb "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twYcPNxzxKhN"
      },
      "source": [
        "## **Download Kaggle Dataset and Dataframe create**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spkVEHm1mDfc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imutils import paths\n",
        "from google.colab import files\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9mnoKdZl330"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggpxwBB6xNq7"
      },
      "source": [
        "**Dataset Download and unzip**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjZ-2UFdl8f0"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d trolukovich/apparel-images-dataset\n",
        "!mkdir dataset\n",
        "!unzip -q apparel-images-dataset.zip -d ./dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QyMr5q2xSeL"
      },
      "source": [
        "**Dataframe Shuffle and Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjekI0J2mHYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27d594d-e626-44c1-f30e-291f6d189589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9108\n",
            "1821\n",
            "456\n"
          ]
        }
      ],
      "source": [
        "df_apparel = pd.read_csv(\"/content/df_apparel_multilabel.csv\")\n",
        "df = shuffle(df_apparel)\n",
        "ulb_dataframe , lb_dataframe = train_test_split(df,test_size = 0.2)\n",
        "lb_dataframe = shuffle(lb_dataframe)\n",
        "lb_dataframe, valid_dataframe = train_test_split(lb_dataframe,test_size = 0.2)\n",
        "print(len(ulb_dataframe))\n",
        "print(len(lb_dataframe))\n",
        "print(len(valid_dataframe))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfYfBa6-a2Z_"
      },
      "source": [
        "## **Useful imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj0YwBeqB3on"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from timm.data.auto_augment import rand_augment_transform\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import timm\n",
        "import time \n",
        "from collections import OrderedDict\n",
        "\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxbdHckhycGb"
      },
      "source": [
        "**Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl_QPuYTmVy7"
      },
      "outputs": [],
      "source": [
        "# device is set to cuda if cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "num_epochs = 10\n",
        "total_class = len(df_apparel.columns)-1\n",
        "threshold = 0.90\n",
        "learning_rate = 0.001\n",
        "lb_to_ulb_ratio = 4\n",
        "batch_size_lb = 6\n",
        "Color_logits = 6\n",
        "save_path_checkpoints= \"/content/model/ckpts\"\n",
        "os.makedirs(save_path_checkpoints, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ud6TU5xyd5i"
      },
      "source": [
        "**Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctbcq1q2BJFu"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe,weak_transform,strong_transform,normalize,is_lb=False):\n",
        "        self.dataframe = dataframe\n",
        "        self.weak_transform = weak_transform\n",
        "        self.strong_transform = strong_transform\n",
        "        self.normalize = normalize\n",
        "        self.is_lb = is_lb\n",
        "        self.all_image_names = self.dataframe[:]['ImagePath']\n",
        "        self.all_image_label = self.dataframe.drop(['ImagePath'], axis=1)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.all_image_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.all_image_names.iloc[index])\n",
        "        image = PIL.Image.open(img_path)\n",
        "        if self.is_lb == True:\n",
        "          targets = np.array(self.all_image_label.iloc[index],dtype = np.float32)\n",
        "          return self.normalize(image),targets\n",
        "        else:  \n",
        "          weak_image = self.weak_transform(image)\n",
        "          strong_image = self.strong_transform(image)\n",
        "          return self.normalize(weak_image),self.normalize(strong_image)\n",
        "         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBODZ7_6yf1a"
      },
      "source": [
        "**Dataloader and Transfrom**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38Xf_3C9ojx1"
      },
      "outputs": [],
      "source": [
        "def fun_transfrom():\n",
        "  strong_transform =  rand_augment_transform(\n",
        "    config_str='rand-m9-mstd0.5', \n",
        "    hparams={}\n",
        "              )\n",
        "\n",
        "  weak_transform = transforms.Compose([transforms.RandomHorizontalFlip()])\n",
        "      \n",
        "      \n",
        "  normalize = transforms.Compose([\n",
        "            transforms.Resize((224,224)),                      \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "\n",
        "  ulb_dataset = ImageDataset(\n",
        "      ulb_dataframe,\n",
        "      weak_transform,\n",
        "      strong_transform,\n",
        "      normalize,\n",
        "      is_lb=False\n",
        "      \n",
        "  )\n",
        "  lb_dataset = ImageDataset(\n",
        "      lb_dataframe,\n",
        "      weak_transform,\n",
        "      strong_transform,\n",
        "      normalize,\n",
        "      is_lb=True\n",
        "      \n",
        "  )\n",
        "\n",
        "  valid_dataset = ImageDataset(\n",
        "      valid_dataframe,\n",
        "      weak_transform,\n",
        "      strong_transform,\n",
        "      normalize,\n",
        "      is_lb=True\n",
        "      \n",
        "  )\n",
        "  \n",
        "  dataloader_ulb_dataset = DataLoader(ulb_dataset, batch_size=batch_size_lb * lb_to_ulb_ratio, shuffle=True, num_workers=2)\n",
        "  dataloader_lb_dataset = DataLoader(lb_dataset, batch_size=batch_size_lb, shuffle=True, num_workers=2)\n",
        "  dataloader_valid_dataset = DataLoader(valid_dataset, batch_size=len(valid_dataframe), shuffle=False, num_workers=2)\n",
        "\n",
        "  return dataloader_ulb_dataset,dataloader_lb_dataset,dataloader_valid_dataset\n",
        "\n",
        "\n",
        "dataloader_ulb_dataset,dataloader_lb_dataset,dataloader_valid_dataset = fun_transfrom()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq1OJj0Fb7uV"
      },
      "source": [
        "**Wandb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M97VqEEMzf_"
      },
      "outputs": [],
      "source": [
        "!wandb login "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3YKEACgfYzp"
      },
      "outputs": [],
      "source": [
        "class WandbLogger():\n",
        "    \"\"\"\n",
        "    This custom callback is used for logging training metrics to wandb for monitoring.\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self,project,entity,name,id,config,resume = \"allow\",):\n",
        "      self.project = project\n",
        "      self.entity = entity\n",
        "      self.name = name\n",
        "      self.id = id\n",
        "      self.config = config\n",
        "      self.resume = resume\n",
        "      wandb.init(project = self.project,entity = self.entity,\n",
        "                 name = self.name,id = self.id, \n",
        "                 config = self.config,resume = self.resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6pk20PrcAPR"
      },
      "outputs": [],
      "source": [
        "project= PROJECT_NAME\n",
        "entity=\"rakib1521\"\n",
        "\n",
        "\n",
        "name = f\"{PROJECT_NAME}_{ARCHITECTURE_NAME}\" #same name for multiple run is allowed but same id is not allowed\n",
        "id = f\"{PROJECT_NAME}_{ARCHITECTURE_NAME}\"\n",
        "\n",
        "wandb_config = {\"network\":ARCHITECTURE_NAME,\n",
        "                \"epoch\":num_epochs,\n",
        "                \"batch_size_lb\": batch_size_lb,\n",
        "                \"lb_to_ulb_ratio\":lb_to_ulb_ratio,\n",
        "                \"learning_rate\": learning_rate,\n",
        "                \"probability_threshold\": threshold,\n",
        "                }\n",
        "wandb_logger = WandbLogger(project,entity,name,id,wandb_config)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV6ttB1Lym7u"
      },
      "source": [
        "# Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFS7148AOhR3"
      },
      "outputs": [],
      "source": [
        "model = timm.create_model('tf_efficientnet_lite0')\n",
        "\n",
        "classifier = torch.nn.Sequential(OrderedDict([\n",
        "    ('fc1', torch.nn.Linear(model.classifier.in_features, total_class))\n",
        "]))\n",
        "\n",
        "model.classifier = classifier\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkBdFodftUY-"
      },
      "outputs": [],
      "source": [
        "def fun_loss_lb(logits,true):\n",
        "       \n",
        "        ce_loss = nn.CrossEntropyLoss()  \n",
        "\n",
        "\n",
        "        loss_cls1 = ce_loss(logits[:, :Color_logits], true[:, :Color_logits])  # Cross Entropy Error (for color classification)\n",
        "        loss_cls2 = ce_loss(logits[:, Color_logits:], true[:, Color_logits:])  # Cross Entropy Error (for cloth classification)\n",
        "        \n",
        "        total_loss = loss_cls1 + loss_cls2\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmZXXrGya7Ic"
      },
      "outputs": [],
      "source": [
        "def pseudo_label_calc(logits,threshold):\n",
        "        max_prob, pseudo_label = torch.max(logits, dim=1)\n",
        "        \n",
        "        # mask for\n",
        "        mask = (max_prob > threshold).float() # [1, 0] [batch_size,]\n",
        "               \n",
        "        # mask non-confident prediction\n",
        "        pseudo_label = pseudo_label.masked_fill(mask == 0, 0) # [3, 0, ...]\n",
        "        \n",
        "\n",
        "        return pseudo_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ohobVryabV9"
      },
      "outputs": [],
      "source": [
        "def fun_loss_ulb(logits_weak,logits_strong,threshold):\n",
        "        \n",
        "        logits_color = F.softmax(logits_weak[:,:Color_logits], dim=1)\n",
        "\n",
        "        pseudo_label_color = pseudo_label_calc(logits_color,threshold)\n",
        "\n",
        "        logits_cloth = F.softmax(logits_weak[:,Color_logits:], dim=1)\n",
        "\n",
        "        pseudo_label_cloth = pseudo_label_calc(logits_cloth,threshold)\n",
        "\n",
        "      \n",
        "        \n",
        "        \n",
        "\n",
        "        ce_loss = nn.CrossEntropyLoss()  \n",
        "\n",
        "\n",
        "        loss_color = ce_loss(logits_strong[:,:Color_logits], pseudo_label_color)\n",
        "        loss_cloth = ce_loss(logits_strong[:,Color_logits:], pseudo_label_cloth)\n",
        "\n",
        "\n",
        "\n",
        "        return loss_color + loss_cloth "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMHARNntIN0g"
      },
      "outputs": [],
      "source": [
        "def train_fixMatch(threshold):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    pbar = tqdm(dataloader_lb_dataset)\n",
        "    \n",
        "    ulb_iter = iter(dataloader_ulb_dataset)\n",
        "    \n",
        "    for batch_lb in pbar:\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        # labelled data\n",
        "        x_lb, y = batch_lb\n",
        "        x_lb, y = x_lb.to(device), y.to(device)\n",
        "        \n",
        "        # unlabelled data\n",
        "        try:\n",
        "            x_weak, x_strong = next(ulb_iter)\n",
        "        except StopIteration:\n",
        "            ulb_iter = iter(dataloader_ulb_dataset)\n",
        "            x_weak, x_strong = next(ulb_iter)\n",
        "                \n",
        "        x_weak, x_strong = x_weak.to(device), x_strong.to(device)\n",
        "        \n",
        "        # concat all x\n",
        "        all_x = torch.cat([x_lb, x_weak, x_strong], dim=0)\n",
        "        \n",
        "        # compute logits\n",
        "        all_logits = model(all_x)\n",
        "        \n",
        "        # logits and loss for labelled data\n",
        "        logits_lb = all_logits[:x_lb.size(0)]\n",
        "        \n",
        "        #loss_lb = F.cross_entropy(logits_lb, y)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # logits for unlabelled data\n",
        "        logits_ulb = all_logits[x_lb.size(0):]\n",
        "        logits_weak, logits_strong = torch.chunk(logits_ulb, 2, dim=0)\n",
        "\n",
        "        # stop gradient for weak augmented\n",
        "        logits_weak = logits_weak.detach() \n",
        "\n",
        "        \n",
        "        # compute loss \n",
        "        lb_loss = fun_loss_lb(logits_lb,y)\n",
        "\n",
        "        loss = lb_loss + fun_loss_ulb(logits_weak,logits_strong,threshold)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_item = loss.item()\n",
        "        \n",
        "        losses.append(loss_item)\n",
        "        \n",
        "        pbar.set_description(f'train loss = {np.array(losses).mean(): .3f}')\n",
        "        \n",
        "    return np.array(losses).mean()\n",
        "@torch.no_grad()\n",
        "def validate():\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    labels_all = []\n",
        "    logits_all = []\n",
        "    \n",
        "    \n",
        "    for x, y in dataloader_valid_dataset:\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        \n",
        "    \n",
        "    acc_color = accuracy_score(logits[:, :Color_logits].argmax(axis=1).tolist(),y[:, :Color_logits].argmax(axis=1).tolist())\n",
        "    acc_cloth = accuracy_score(logits[:, Color_logits:].argmax(axis=1).tolist(),y[:, Color_logits:].argmax(axis=1).tolist())\n",
        "   \n",
        "    print('acc_color- {}, acc_cloth- {}'.format(acc_color,acc_cloth))\n",
        "                    \n",
        "    return acc_color,acc_cloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWyJohbrInek"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "accuracies_color = []\n",
        "accuracies_cloth = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_fixMatch(threshold=threshold)\n",
        "    acc_color,acc_cloth = validate()\n",
        "    accuracies_color.append(acc_color)\n",
        "    accuracies_cloth.append(acc_cloth)\n",
        "\n",
        "    losses.append(train_loss)\n",
        "    \n",
        "    wandb.log({ \"train_loss\": train_loss,\n",
        "                \"valid_acc_color\":acc_color,\n",
        "                \"valid_acc_cloth\":acc_cloth})    \n",
        "\n",
        "             \n",
        "    filepath=f\"{save_path_checkpoints}/{PROJECT_NAME}_{MODEL_TYPE}-{ARCHITECTURE_NAME}-{epoch+1}_loss-{train_loss}.pt\"    \n",
        "    checkpoint= {\n",
        "                    \"epoch\" : epoch+1 ,\n",
        "                    \"model_weight\" : model.state_dict(),\n",
        "                    \"optimizer_state\" : optimizer.state_dict()\n",
        "                }\n",
        "    torch.save(checkpoint,filepath)\n",
        "    print(\"{} saved\".format(filepath))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT_e4kSIotT9"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(num_epochs), losses)\n",
        "plt.title('traning Loss Vs epoch')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('traning Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vv4K-cez3lk"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(num_epochs), accuracies_color)\n",
        "plt.title('Validation accuracy color Vs epoch')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('validation accuracy color')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0eAW08bsEUD"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(num_epochs), accuracies_cloth)\n",
        "plt.title('Validation accuracy scratching Vs cloth')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('validation accuracy cloth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E29AoDkm4P5Q"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate():\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    labels_all = []\n",
        "    logits_all = []\n",
        "    \n",
        "    \n",
        "    for x, y in dataloader_valid_dataset:\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        \n",
        "    \n",
        "    acc_color = accuracy_score(logits[:, :Color_logits].argmax(axis=1).tolist(),y[:, :Color_logits].argmax(axis=1).tolist())\n",
        "    acc_cloth = accuracy_score(logits[:, Color_logits:].argmax(axis=1).tolist(),y[:, Color_logits:].argmax(axis=1).tolist())\n",
        "   \n",
        "    \n",
        "\n",
        "    color_pred = logits[:, :Color_logits].argmax(axis=1).tolist()\n",
        "    color_y = y[:, :Color_logits].argmax(axis=1).tolist()\n",
        "    cloth_pred = logits[:, Color_logits:].argmax(axis=1).tolist()\n",
        "    cloth_y = y[:, Color_logits:].argmax(axis=1).tolist()\n",
        "\n",
        "\n",
        "    print('acc_color- {}, acc_cloth- {}'.format(acc_color,acc_cloth))\n",
        "                    \n",
        "    return    acc_color,acc_cloth\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p50hnAFHo7k0"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "acc = []\n",
        "for num,path in enumerate(sorted(glob.glob(\"/content/model/ckpts/*\"))):\n",
        "  checkpoint = torch.load(path)\n",
        "  model.load_state_dict(checkpoint['model_weight'])\n",
        "  acc_color,acc_cloth  = validate()\n",
        "  acc.append((acc_color+acc_cloth)/2)\n",
        "  #print(path)\n",
        "\n",
        "sorted(glob.glob(\"/content/model/ckpts/*\"))[np.argmax(acc)]  \n",
        "\n",
        "print(\"###\")\n",
        "model.load_state_dict(torch.load(sorted(glob.glob(\"/content/model/ckpts/*\"))[np.argmax(acc)])[\"model_weight\"])\n",
        "acc_color,acc_cloth  = validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vca2nrp-vOD"
      },
      "outputs": [],
      "source": [
        "acc_color,acc_cloth  = validate()\n",
        "\n",
        "wandb.log({ \"Test_acc_color\":acc_color,\n",
        "            \"Test_acc_cloth\":acc_cloth})  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZqH48ZrcwIf"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PCDWuxUBGHkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Fixmatch_Multioutput_Implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}